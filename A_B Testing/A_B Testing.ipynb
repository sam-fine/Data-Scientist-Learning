{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f580e72",
   "metadata": {},
   "source": [
    "# A/B testing and Hypothesis Testing\n",
    "\n",
    "https://www.youtube.com/watch?v=DUNk4GPZ9bw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe23557",
   "metadata": {},
   "source": [
    "## Prerequisite: P-Values\n",
    "\n",
    "https://www.youtube.com/watch?v=vemZtEM63GY&list=PLblh5JKOoLUIcdlgu78MnlATeyx4cEVeR&index=25\n",
    "\n",
    "- Comparing two distributions (e.g. comparing two drugs, with different means for how many people it cured):\n",
    "    - H_0 = distributions are the same (drugs are the same)\n",
    "    - H_1 = distributions are different (drugs are different)\n",
    "    \n",
    "- p-values are between 0 and 1.\n",
    "- The closer the p-value is to 0, the more confidence we have in H_1.\n",
    "- Generally, a p-value of 0.05 is used.\n",
    "- This means: \n",
    "    - **if there is no difference between the two distributions (the two drugs), and if we did the experiment many times, then only 5% of those experiments would result in the wrong decision (saying they are different)**\n",
    "\n",
    "### Same Distribution - usually large p-values:\n",
    "    \n",
    "![p_val_same_dist](p_values_same_dist_high_value.png)\n",
    "    \n",
    "### Same distribution - 5% of the time small p-values\n",
    "\n",
    "![p_val_same_dist_low](p_values_same_dist_low_value.png)   \n",
    "\n",
    "So, if there is no difference between the two drugs (H_0 true), 5% of the time we do the experiment, we will get a p-value less than 0.05, and we would incorrectly reject H_0. (false positive).\n",
    "\n",
    "If we perform the experiment, and the p-value < 0.05, we decide the drugs are different.\n",
    "\n",
    "**Note: p-value variations:**\n",
    "- If extremely important to correctly conclude the drugs are different, we can use smaller thresholds for the p-value, e.g. 0.00001. (1 out of 100,000 experiments we get false positive)\n",
    "- The opposite is also true, if correct conclusions of rejecting H_0 and accepting H_1 are not important, we can have large p-values, e.g. 0.2.\n",
    "\n",
    "**Note: how different the distributions actually are to each other:**\n",
    "- A small p-value helps us decide if the drugs are different, but does not tell us **how different (effect size)** they are.\n",
    "- We can have small p-values but with tiny or huge distribution differences\n",
    "    - The size of the sample changes p-values, and how correlated p-value is to effect size.\n",
    "    - The percentage cured may be very different and still have a large p-value\n",
    "    \n",
    "### p-value calculation\n",
    "\n",
    "**1. A type of distribution is chosen for the hypothesis test (e.g. t-test/ normal)**\n",
    "\n",
    "####  Choosing the Right Test and Distribution\n",
    "\n",
    "| What You‚Äôre Testing            | Distribution              | Common Test                        | Notes                                  |\n",
    "|-------------------------------|---------------------------|------------------------------------|----------------------------------------|\n",
    "| Mean (œÉ unknown, small n)     | **t-distribution**        | One-sample or two-sample **t-test**| Small sample or population œÉ unknown   |\n",
    "| Mean (œÉ known or large n)     | **Normal (Z) distribution** | **Z-test**                        | Use when population œÉ is known or n is large |\n",
    "| Proportions                   | **Normal (Z) distribution** | **Proportion Z-test**             | Use when sample size is large (normal approximation) |          |\n",
    "| Categorical data              | **Chi-squared distribution** | **Chi-square test**             | Test of independence                   |\n",
    "| Variances (between groups)    | **F-distribution**         | **ANOVA**, variance comparison     | Comparing multiple group variances     |\n",
    "\n",
    "**Note:**\n",
    "- 2-sampled t-test is used when there is no population distribution and we have two independent samples and we want to compare means. (assuming both groups have equal variance, otherwise use Welch's test).\n",
    "- 1-sample is when we want to compare the mean of a single sample to a known population mean (not knowing population standard deviation).   \n",
    "\n",
    "**2. A distribution score (test statistic) is calculated (e.g. t-score/z-score), telling you:**\n",
    "- How many standard deviations your sample result is away from the mean that is expected under the null hypothesis (negative or positive) .\n",
    "e.g.\n",
    "\n",
    "**t-score:**\n",
    "\n",
    "- Use when you have small sample or s.d is unknown. \n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\bar{x}$ = sample mean  \n",
    "- $\\mu_0$ = population mean under the null hypothesis  \n",
    "- $s$ = sample standard deviation (use population s.d for normal distribution)\n",
    "- $n$ = sample size\n",
    "\n",
    "\n",
    "**Note:** a t-distribution is similar to a z/normal - distribution but it has fatter tails because samples are smaller so rare cases are more likely. The larger the sample size in t-test, the thinner the tails and the closer it is to the normal distribution. \n",
    "\n",
    "### Degrees of Freedom\n",
    "\n",
    "When calculating the p-value for a t-test, you need to know the degrees of freedom, as this is an input into the python formula.\n",
    "\n",
    "> **Degrees of freedom** = number of independent values that can vary without breaking a constraint.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß Everyday Analogy\n",
    "\n",
    "- Packing 5 items with total weight = 100 kg\n",
    "- You choose weights for 4 items freely\n",
    "- The 5th item‚Äôs weight is fixed to make total 100\n",
    "\n",
    "**Degrees of freedom** = 5 (items) ‚àí 1 (constraint) = 4\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä In Statistics\n",
    "\n",
    "Sample variance formula:\n",
    "\n",
    "\n",
    "$$s^2 = \\frac{1}{n-1} \\sum (x_i - \\bar{x})^2$$\n",
    "\n",
    "\n",
    "Why divide by \\(n-1\\) instead of \\(n\\)?\n",
    "\n",
    "- After calculating the sample mean, one value is fixed.\n",
    "- Only \\(n-1\\) values can vary freely.\n",
    "\n",
    "**Degrees of freedom** = \\(n - 1\\)\n",
    "\n",
    "- This gives an unbiased estimate of the population variance.\n",
    "- Mostly relevant when n is small, like in t-tests.\n",
    "\n",
    "\n",
    "**Note:** The z-score is the same as above but we use the population standard deviation rather than the sample, as we have a large sample size. \n",
    "\n",
    "$$\n",
    "z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "### Division by $\\sqrt{n}$:\n",
    "\n",
    "Because of the square root in the denominator:\n",
    "\n",
    "$\\sqrt{n}$ grows **slower** than  $n$ ‚Äî this is **non-linear growth**.\n",
    "\n",
    "This means that:\n",
    "- Gains in precision (i.e., smaller standard error - the denominator) **get smaller and smaller** as your sample size increases\n",
    "- This phenomenon is called **diminishing returns** in sampling\n",
    "\n",
    "![root_n](root_n.png)\n",
    "\n",
    "#### üìâ Intuition:\n",
    "- Small \\( n \\) ‚Üí each new data point **greatly reduces error**\n",
    "- Large \\( n \\) ‚Üí each new data point **only slightly improves precision**\n",
    "\n",
    "**3. The score is translated to a percentage (the p-value), representing what percentage of the distribution is outside the stated number of standard deviations.**\n",
    "\n",
    "- Use normal distribution/ z-score when sample is large and s.d is known\n",
    "\n",
    "![p_value_on_graph](p_value_on_graph.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e963d156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score = -2.5\n",
      "p_value for 1 tailed test with normal distribution: 0.006209665325776159\n",
      "p_value for 2 tailed test with normal distribution: 0.012419330651552318\n"
     ]
    }
   ],
   "source": [
    "# Use scipy to get p-value (normal distribution):\n",
    "from scipy.stats import norm\n",
    "z_score = -2.5\n",
    "print(f'z-score = {z_score}')\n",
    "# 1-tailed dist (like above)\n",
    "p_value = (1 - norm.cdf(abs(z_score)))\n",
    "print(f'p_value for 1 tailed test with normal distribution: {p_value}')\n",
    "# 2-tailed dist (H_1: distributions are different and the mean(or statistic) is either larger or smaller than the original)\n",
    "p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
    "print(f'p_value for 2 tailed test with normal distribution: {p_value}')\n",
    "\n",
    "## norm.cdf uses the cumulative distribution function F(x)=P(X‚â§x) (based on the Gaussian equation)\n",
    "## The probability that the variable X (random variable) takes on a value less than or equal to a value x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470959a2",
   "metadata": {},
   "source": [
    "## A/B Test Purpose\n",
    "\n",
    "To determine whether a change in a metric is because of random chance or because of the change you have implemented. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c13603",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "What is the business goal of the experiment. What is the success metric? Define User funnel.  \n",
    "**Choosing a success metric:**\n",
    "- **Measurable:** can it be measured with the data we have?\n",
    "- **Attributable:** Can you assign the behaviour (effect of change) to the treatment(cause - the actual change)\n",
    "- **Sensitive:** Does the metric have low variability, meaning it will be relatively consistent, so we can distinguish between the treatment and the control group. \n",
    "- **Timely:** Can the success metric be measured in short-term (not cost effective if it takes a long time to measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2abe54",
   "metadata": {},
   "source": [
    "## üß™ Problem for Our A/B Test\n",
    "**Business Goal:** Increase the conversion rate from clicking ‚ÄúGet Started‚Äù and viewing the Get Started page to completing the Get Started questionnaire. (Improving the CTA - Call To Action button)  \n",
    "**Product:** OurRitual Website - Couples therapy   \n",
    "**Test Feature:** Change the button label from ‚ÄúGet Started‚Äù to something more emotionally engaging, e.g., ‚ÄúBegin Your Journey‚Äù   \n",
    "**Success Metric:** Conversion rate   \n",
    "\n",
    "**Version A:** Get Started button  \n",
    "**Version B:** Begin Your Journey button\n",
    "\n",
    "Success Metric:\n",
    "$$Conversion\\_rate =  \\frac{total\\_questionnaire\\_completions}{total\\_landings\\_on\\_page}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464958b2",
   "metadata": {},
   "source": [
    "## User Funnel:\n",
    "\n",
    "| Step | Funnel Stage Description              | Conversion Event                                    |\n",
    "|------|----------------------------------------|----------------------------------------------------|\n",
    "| 1Ô∏è‚É£   | Landing on the Homepage or Ad          | User arrives via ad, organic link, etc.            |\n",
    "| 2Ô∏è‚É£   | Click ‚ÄúGet Started‚Äù CTA                | User clicks ‚ÄúGet Started‚Äù or equivalent CTA        |\n",
    "| 3Ô∏è‚É£   | Views Get Started Page                 | User is routed to the onboarding form/questionnaire|\n",
    "| 4Ô∏è‚É£   | Begins Questionnaire (optional)        | User interacts with the form (e.g., fills first field) |\n",
    "| 5Ô∏è‚É£   | Completes Questionnaire ‚úÖ             | User submits all answers and completes onboarding  |\n",
    "\n",
    "Our Success Metric is the conversion from step 3 to step 5. \n",
    "\n",
    "## User Funnel Example Marketing Example: (funnels down)\n",
    "\n",
    "\n",
    "![User Funnel](user_funnel.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33510091",
   "metadata": {},
   "source": [
    "## 2. Hypothesis Testing\n",
    "\n",
    "What result do you hypothesise from the experiment?  \n",
    "What is the Null and Alternate Hypotheses.  \n",
    "Set up some parameter values such as the significance level and statistical power    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6328ccc",
   "metadata": {},
   "source": [
    "**Null Hypothesis:**  \n",
    "The new wording for the CTA button has no effect on the questionnaire completion rate:  \n",
    "$$H\\_0: p_{A} = p_{B}$$  \n",
    "**Alternate Hypothesis:**  \n",
    "The new wording for the CTA button increases the questionnaire completion rate:  \n",
    "$$H\\_0: p_{B} > p_{A}$$\n",
    "\n",
    "As the alternate hypothesis is only greater than rather than $\\ne$, this is a one-tailed test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee9a70",
   "metadata": {},
   "source": [
    "\n",
    "**Significance Level (Œ±):** 0.05  \n",
    "\n",
    "**Statistical Power:** 0.8\n",
    "\n",
    "**Minimum Detectable Effect (MDE):** A 2% absolute lift in the conversion rate is meaningful. (e.g. 25%-27%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efb841",
   "metadata": {},
   "source": [
    "## Significance Level\n",
    "\n",
    "- The threshold you set for rejecting the null hypothesis.\n",
    "- If the probability of observing a specific event (our sample data) is very low, then it is statistically significant.\n",
    "- It represents the **Probability of making a Type I Error - rejecting the null hypothesis when it is actually true** (a false positive)\n",
    "- 0.05 means there is a 5% chance of the above\n",
    "- If the p-value is less than 0.05, we reject H0 knowing we have at most a 5% chance of a false positive. \n",
    "\n",
    "![significance_level](significance_level_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e4c4b",
   "metadata": {},
   "source": [
    "## Statistical Power (1-Œ≤)\n",
    "\n",
    "https://www.youtube.com/watch?v=Rsc5znwR5FA&list=PLblh5JKOoLUIcdlgu78MnlATeyx4cEVeR&index=98\n",
    "\n",
    "**What is it?:**\n",
    "- The probability of correctly detecting a real effect/difference is 80% (H1 is true)\n",
    "- Power is the probability we will correctly reject the Null hypothesis, given that H1 is true. (accept H1 when it is true)\n",
    "- It is the probability (Œ≤) of avoiding a **Type II Error - accepting the Null hypothesis when it is actually false** (a false negative)\n",
    "    - So we have 20% chance of a false negative, this is higher than a false positive (type 1 error) as having a false negative usually less influential than having a false positive as we are remaining at what we were before. \n",
    "\n",
    "**Why is it important?**  \n",
    "A study with low statistical power might fail to detect a real effect, leading to a false negative conclusion. Conversely, a study with high power is more likely to correctly identify a true effect.\n",
    "\n",
    "**Factors Affecting Power:**\n",
    "- **Effect Size:** Larger effect sizes are easier to detect, increasing power. \n",
    "- **Sample Size:** Larger sample sizes reduce variability and increase power. \n",
    "- **Significance Level (Alpha):** A higher alpha (e.g., 0.05) increases power but also increases the risk of a Type I error (false positive). \n",
    "- **Variability:** Lower variability in the data increases power. \n",
    "\n",
    "\n",
    "When we are comparing two distributions that have little overlap, the power is highest, as the average of samples taken from each distribution will result in low p-values when comparing the means (large difference in sample means compared to population mean so large test variable and small p-value). This leads to high probability of rejecting H0.   \n",
    "\n",
    "Below, H0 is that the distributions are the same and H1 is they are different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bed5ba",
   "metadata": {},
   "source": [
    "## High Statistical Power\n",
    "![High Statistical Power](statistical_power_little_overlap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3078500",
   "metadata": {},
   "source": [
    "- If distributions overlap a lot and we have a small sample size, the power will be low\n",
    "- This can happen when the chosen 'Test Feature' makes a small change to the success metric.\n",
    "- In this case, the p-value is likely to be high, and thus more likely to accept H0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52342c85",
   "metadata": {},
   "source": [
    "## Low Statistical Power\n",
    "![High Statistical Power](statistical_power_large_overlap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acf3dc",
   "metadata": {},
   "source": [
    "Watch power analysis: https://www.youtube.com/watch?v=VX_M3tIyiYk&list=PLblh5JKOoLUIcdlgu78MnlATeyx4cEVeR&index=95\n",
    "\n",
    "Used to detemine the appropriate sample size (uses statistical power and MDE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bc21b",
   "metadata": {},
   "source": [
    "## P-hacking\n",
    "\n",
    "https://www.youtube.com/watch?v=HDCOUXE3HMM\n",
    "\n",
    "- p-hacking refers to the misuse and abuse of analysis techniques and results in being fooled by false positives.  \n",
    "\n",
    "**How to avoid False Positives?**\n",
    "\n",
    "\n",
    "### 1st p-hack: Multiple Testing Problem:\n",
    "Doing a lot of hypothesis tests, and ending up/focusing on the ones with **False Positives** is called the **Multiple Testing Problem** - p-hacking\n",
    "- With a confidence interval of 5%, we expect that 5% of tests will result in a false positive \n",
    "- There are many ways to reduce the number of false positives (e.g. the False Discovery Rate)\n",
    "- The methods include inputting ALL p-values from ALL tests I do and the method outputs an adjusted p-value, usually larger than the original\n",
    "- Don't cherry pick my tests and only pick ones that look good, use ALL!\n",
    "\n",
    "### 2nd p-hack: Increasing sample size after small p-value calculated:\n",
    "When you have a p-value close to the required, e.g. 0.06 and you want it to be less than 0.05, increasing sample size can decrease the p-value to what we want. e.g. 0.02. - p-hacking\n",
    "- To avoid the above, calculate the sample size **before** the experiment. \n",
    "- This is a **power analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c0374",
   "metadata": {},
   "source": [
    "## Power Analysis\n",
    "\n",
    "https://www.youtube.com/watch?v=VX_M3tIyiYk&list=PLblh5JKOoLUIcdlgu78MnlATeyx4cEVeR&index=96\n",
    "\n",
    "How to avoid the 2nd p-hack (above)\n",
    "\n",
    "### What is it?\n",
    "A **Power Analysis** determines what sample size will ensure a high probability that we **correctly** reject the **Null Hypothesis** that there is no difference between the groups\n",
    "- So, when using the **Sample size** recommended by the **Power Analysis** we know that regardless of the p-value we calculate (e.g. 0.06), we have used enough data to be confident in the result.\n",
    "\n",
    "The main factors effecting the power are explained in statistical power above. The two main ones are:\n",
    "1. **Effect Size:** Difference in final success metrics. How much overlap there is between the two distributions\n",
    "    - The less overlap (e.g. the change made a big difference), the larger the power\n",
    "    - The more overlap, the smaller the power\n",
    "2. **The Sample Size:** the number of measurements we collect from each group\n",
    "    - The larger the sample size, the larger the power\n",
    "    - the smaller the sample size, the smaller the power\n",
    "    \n",
    "Combinations of the two factors above can provide the optimal power, e.g. 0.8 or 80%. (probability of accepting H1 when it is true)\n",
    "- If we have **more overlap**, we a can **increase the sample size**\n",
    "- If we have a **small sample size**, we can **decrease the overlap**\n",
    "\n",
    "The more measurements we use to estimate the population mean (larger sample) extreme measurements have less effect on how far the **estimated mean** is from the **Population mean**.  \n",
    "So the more measurements we have for the estimated mean, the more confidence we have in the estimated mean\n",
    "\n",
    "### Performing a Power Analysis\n",
    "Aim is to find the sample size needed to achieve a defined power.\n",
    "1. Decide how much **Power (1-Œ≤)** we want \n",
    "    - 0.8 is common: 80% chance of correctly rejecting the Null hypothesis \n",
    "2. Determine the **Significance Level** (alpha)\n",
    "    - 0.05 is common: 5% chance of incorrectly accepting the null hypothesis (False negative)\n",
    "3. Estimate the overlap between the two distributions (can use MDE)\n",
    "    - Effected by the difference in the population **means** AND the **Standard Deviations**\n",
    "    - One way to combine the above, is to calculate an **Effect Size (d)** (many ways to do this including below)\n",
    "    - Generally, the mean and standard deviations can be estimated with prior data/ research\n",
    "![effect](effect_size.png)\n",
    "4. Plug in the Power, significance level and the effect size into a statistical power calculater and find the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e938e75",
   "metadata": {},
   "source": [
    "## Minimum Detectable Effect (MDE) and Lift\n",
    "\n",
    "**What is it?:**\n",
    "- The smallest change in the success metric to make it **Practically Significant**.\n",
    "- Defines the sensitivity of the experiment, indicating the smallest effect size needed for practical significance.\n",
    "\n",
    "**Statistically Significant vs Practically Significant:**\n",
    "- The experiment may be statistically significant, with a small p-value (reject H0), but then not practically significant as the change in the success metric is small (accept H0). \n",
    "- Here, we would reject H0 but we would not make the change\n",
    "\n",
    "**Lift:**\n",
    "- The practical improvement of the success metric (e.g. conversion rate)\n",
    "- You compare the lift to the MDE.\n",
    "\n",
    "| üìä **Lift Type**     | üìê **Formula**                                       | üí° **Example Calculation**          |\n",
    "|----------------------|------------------------------------------------------|-------------------------------------|\n",
    "| üî∏ **Absolute Lift** | `Conversion_B ‚àí Conversion_A`                        | `22% ‚àí 20% = 2%`                    |\n",
    "| üîπ **Relative Lift** | `(Conversion_B ‚àí Conversion_A) / Conversion_A`       | `(22% ‚àí 20%) / 20% = 10%`           |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd91e2e",
   "metadata": {},
   "source": [
    "## 3. Design the Experiment\n",
    "\n",
    "What are your experiment paramaters?  \n",
    "What is the randomisation unit?  \n",
    "Which user type will we target for the experiment?  \n",
    "\n",
    "1. Set the **Randomisation Unit:** Assigning users to treatment group or control group\n",
    "2. **Target Population:** (visitors who clicked the CTA button - 3 in the user funnel)\n",
    "3. Determine **Sample Size**: \n",
    "    - Can use power analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f13f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group: 7548\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# Define baseline conversion rate and desired minimum effect\n",
    "p1 = 0.25  # baseline conversion rate\n",
    "p2 = 0.27  # expected improved rate (MDE = 2%)\n",
    "\n",
    "# Calculate effect size (Cohen's h)\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "# Set parameters\n",
    "alpha = 0.05  # significance level\n",
    "power = 0.8   # statistical power\n",
    "\n",
    "# Perform sample size calculation (per group)\n",
    "analysis = NormalIndPower()\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=1)\n",
    "\n",
    "print(f\"Required sample size per group: {round(sample_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b522be6",
   "metadata": {},
   "source": [
    "- **proportion_effectsize()** computes the standardized difference in proportions (Cohen‚Äôs h).\n",
    "    -  It converts the raw difference between two proportions (e.g., a conversion rate of 10% vs. 15%) into a standardized unit, making it comparable across different experiments. \n",
    "\n",
    "- **solve_power()** tells you how many users per group are needed to detect that difference with given alpha/power.\n",
    "    - ratio = 1 assigns equal ratio to the control and the varient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76326f",
   "metadata": {},
   "source": [
    "## 4. Run the Experiment\n",
    "\n",
    "Reuirements for running the experiment?\n",
    "Implementation to collect data and analyse the result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3eef05",
   "metadata": {},
   "source": [
    "## 5. Validity Checks\n",
    "\n",
    "Did the experiment run soundly without errors or bias?\n",
    "Sanity check before launching decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f807c0e",
   "metadata": {},
   "source": [
    "## 6. Interpret the result\n",
    "\n",
    "In which direction is the metric significant statistically and practically?\n",
    "What is the lift that is saw?\n",
    "What is the p-value confidence interval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2314c1",
   "metadata": {},
   "source": [
    "## Launch Decision\n",
    "\n",
    "Based off the results, should the change be launched?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1437ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hello</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bye</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>morning</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>evening</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a        b      c\n",
       "0  1    hello   True\n",
       "1  2      bye   True\n",
       "2  3  morning  False\n",
       "3  4  evening  False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb13792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
