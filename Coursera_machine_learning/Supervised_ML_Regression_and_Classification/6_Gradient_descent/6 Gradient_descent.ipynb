{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e549826c",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4d69b",
   "metadata": {},
   "source": [
    "> **Gradient Descent:** An algorithm you can use to minimise any function over any number of parameters\n",
    "\n",
    "Used to find the minimum of the cost function $J(w,b)$\n",
    "1. Start with some w, b\n",
    "    - set w=0, b=0\n",
    "2. Keep changing w,b to reduce $J(w,b)$ until we settle on or near a minimum\n",
    "    - Keep moving in the **direction of steepest descent**\n",
    "\n",
    "**Note:** For non quadratic cost functions, there can be more than one global minimum \n",
    "\n",
    "![non_quad](grad_descent_non_quadratic.png)\n",
    "You can see above the algorithm taking baby steps in the direction of steepest descent until it reaches a minimum.  \n",
    "**Note:** If you start at different initial conditions for w and b, you can end up at a different local minima.\n",
    "\n",
    "> **Local Minima:** a point in a function's graph where the function's value is the lowest compared to its immediately surrounding values, even if it's not the absolute lowest value the function can reach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743adc1",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf56ee",
   "metadata": {},
   "source": [
    "> **Learning Rate (ùõº):** How big a step we take along the gradient on each iteration between 0 and 1. \n",
    "\n",
    "$$\n",
    "w = w - ùõº\\frac{\\partial J}{\\partial w}  \n",
    "$$\n",
    "$$\n",
    "b = b - ùõº\\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "Repeat the iterations until **convergence**, or until the parameters w and b no longer change much.\n",
    "\n",
    "\n",
    "\n",
    "**Larger ùõº:** reach the minimum faster, but risk overshooting or diverging.  \n",
    "**Smaller ùõº:** more precise convergence but takes longer.  \n",
    "\n",
    "### Diverging with large learning rate\n",
    "![div](diverging.png)\n",
    "\n",
    "- As we approach a minimum in gradient descent, each iteration will get smaller and smaller as the partial derivative gets closer to 0. \n",
    "\n",
    "\n",
    "![small_steps](smaller_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f78414",
   "metadata": {},
   "source": [
    "### Contour Gradient Descent\n",
    "![cont_desc](contour_grad_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9bafd",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "We perform the algorithm over ALL the training data for each step.  \n",
    "There are other variations that use a subset of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
