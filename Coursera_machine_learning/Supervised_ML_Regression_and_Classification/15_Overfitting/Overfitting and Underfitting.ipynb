{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11773e8b",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9fbda",
   "metadata": {},
   "source": [
    "## Linear Regression Example\n",
    "\n",
    "![under_over](overfitting_underfitting_lin_reg_eg.png)\n",
    "\n",
    "### Overfitting vs. Underfitting\n",
    "\n",
    "- **Underfitting (High Bias)**  \n",
    "  - The algorithm is unable to fit the training data well.  \n",
    "  - A clear pattern exists in the training data, but the model cannot capture it.  \n",
    "  - Example: Linear regression applied when the true relationship is polynomial.  \n",
    "  - Cause: Too few features, or the model is too simple.  \n",
    "  - Interpretation: The algorithm has a **bias** that the function is linear, even though the data suggests otherwise.\n",
    "\n",
    "- **Overfitting (High Variance)**  \n",
    "  - The algorithm fits the training data *too well* (even achieving zero cost).  \n",
    "  - The curve is too complex, wiggling to pass through all pointsâ€”including noise.  \n",
    "  - This results in poor generalization to new examples.  \n",
    "  - Cause: Too many features, or an overly flexible model.  \n",
    "  - Another term: **High Variance**  \n",
    "    - The model adapts too strongly to training data variations.  \n",
    "    - Small changes in the training data lead to very different final predictions.\n",
    "    \n",
    "- **Good Generalization (middle)**  \n",
    "  - The learning algorithm balances bias and variance.  \n",
    "  - It makes accurate predictions on both training and unseen data.  \n",
    "  - Example: Middle graph shows a well-generalized model.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Point  \n",
    "The aim of Machine Learning is to find a model that:  \n",
    "- **Does not underfit** (low bias)  \n",
    "- **Does not overfit** (low variance)  \n",
    "- **Generalizes well** to new data\n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression / Classification Example\n",
    "\n",
    "![log_under_over](overfitting_underfitting_logistic_reg_eg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e32f97",
   "metadata": {},
   "source": [
    "# Addressing Over or Under Fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df7484",
   "metadata": {},
   "source": [
    "## Addressing Overfitting\n",
    "\n",
    "There are several strategies to reduce overfitting and improve generalisation:\n",
    "\n",
    "1. **Collect More Training Data**  \n",
    "   - More diverse and representative data helps the model learn general patterns.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Use Fewer Features**\n",
    "\n",
    "   - **Feature Selection**  \n",
    "     - Select only the most relevant features (possibly using intuition or statistical methods).  \n",
    "     - This effectively sets less important features to zero.  \n",
    "     - **Disadvantage:** You might discard features that are actually useful.\n",
    "\n",
    "   - **Use Fewer Polynomial Features**  \n",
    "     - Reducing the degree of polynomial features prevents the model from becoming too complex and wiggly.  \n",
    "     - This lowers variance and improves generalisation.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Regularisation**  \n",
    "\n",
    "   - Instead of removing features entirely, **regularisation** reduces the impact of some features.  \n",
    "   - It encourages the algorithm to **restrict parameter values** (*weights* $w_{j}$), but without forcing them to be exactly zero (as in feature selection).  \n",
    "   - This means we **keep all features** but limit the effect of some.  \n",
    "   - In practice, we usually regularise the $w_{j}$ parameters rather than $b$.  \n",
    "   - Regularising $b$ typically makes little difference, but the main focus is on controlling the weights.\n",
    "\n",
    "\n",
    "![regularisation](regularisation.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
