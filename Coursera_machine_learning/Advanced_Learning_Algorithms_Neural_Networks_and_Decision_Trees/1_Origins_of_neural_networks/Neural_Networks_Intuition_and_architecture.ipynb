{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615a0cd0-fbc6-4434-9830-4a0f7814ce54",
   "metadata": {},
   "source": [
    "# Neurons and the Brain: Origins of Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc128af-878d-4720-b9f3-d64bd0d02b29",
   "metadata": {},
   "source": [
    "## Origins\n",
    "- Original idea: create software that could mimic how the human brain operates.  \n",
    "- Today, many AI neural networks actually work very differently from how the biological brain works.  \n",
    "- We still donâ€™t fully understand how the human brain works.  \n",
    "\n",
    "### Timeline\n",
    "- **1950s** â€“ Early beginnings of neural networks  \n",
    "- **1980s/1990s** â€“ Found popularity  \n",
    "- **2005 onwards** â€“ Resurgence with **deep learning**  \n",
    "\n",
    "### Applications\n",
    "- Speech recognition  \n",
    "- Computer vision  \n",
    "- Natural Language Processing (NLP)  \n",
    "- Many more areas of Machine Learning  \n",
    "\n",
    "---\n",
    "\n",
    "## Biological Inspiration\n",
    "- In the brain, **neurons** send electrical impulses to other neurons.  \n",
    "- Each neuron aggregates inputs from other neurons.  \n",
    "\n",
    "![bio](biological_comparison.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Artificial Neural Networks\n",
    "- Neural networks use a **simplified mathematical model** of a neuron.  \n",
    "- A **circle** represents a neuron:  \n",
    "  - Takes one or more inputs (numbers)  \n",
    "  - Performs a computation  \n",
    "  - Outputs another number (which can be input to another neuron)  \n",
    "\n",
    "### Simulating Many Neurons\n",
    "- Instead of building one neuron at a time, we simulate **many neurons simultaneously**.  \n",
    "- All neurons take inputs, compute, and output numbers that feed into other neurons.  \n",
    "\n",
    "## Big Data and Neural Network Scaling\n",
    "- With the progression of **Big Data**, traditional ML and early AI models struggled to produce good enough performance.  \n",
    "- Neural networks scaled to handle this challenge:  \n",
    "  - **Small networks** â†’ modest performance improvements  \n",
    "  - **Medium networks** â†’ better performance with more capacity  \n",
    "  - **Large networks** â†’ significant performance gains with massive datasets  \n",
    "- This scaling helped unlock the breakthroughs seen in **deep learning** today.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365852c8-c745-43fc-b4eb-fc06c5948521",
   "metadata": {},
   "source": [
    "# Demand Prediction: Neural Network Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742774b-56d0-4ef2-b3d0-1dac6c7af91e",
   "metadata": {},
   "source": [
    "![demand_pred](demand_prediction.png)\n",
    "\n",
    "- Finding the demand of t-shirts being sold in a shop\n",
    "\n",
    "### Logistic Regression as a Neuron\n",
    "- The logistic regression example above can be seen as a **single neuron**:  \n",
    "  - Inputs: **x** (features)  \n",
    "  - Output: **a** (activation, probability of a top-seller)  \n",
    "\n",
    "### Activation Terminology\n",
    "- We can change our function **f(x)** into an **activation (a)**.  \n",
    "- **Activation (a)** is a term from neuroscience, referring to how strongly a neuron sends its output to other neurons.  \n",
    "\n",
    "### Neuron as a Tiny Computer\n",
    "- Another way to think of a neuron:  \n",
    "  - A **tiny little computer** whose only job is:  \n",
    "    - Take one or more numbers **x** as input  \n",
    "    - Perform a computation  \n",
    "    - Output one or more numbers **a**  \n",
    "\n",
    "### Building Neural Networks\n",
    "- A **neural network** is created by wiring together many such neurons.  \n",
    "- Each neuron passes activations forward, allowing the network to learn complex patterns. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da5e72-ecfa-421a-90a8-5b7a6e5bdd61",
   "metadata": {},
   "source": [
    "### More complex example:\n",
    "![layer_eg](demand_prediction_layers.png)\n",
    "\n",
    "### Feature-Neuron Mapping\n",
    "- Different contributing features affect the probability of being a top seller.  \n",
    "- We categorize features into separate neurons, each an **individual logistic regression unit**:\n",
    "  - **Affordability neuron**: function of price and shipping cost  \n",
    "  - **Awareness neuron**: function of marketing  \n",
    "  - **Perceived quality neuron**: function of price (as a signal of quality) and material  \n",
    "\n",
    "- The outputs of these three neurons are wired into another neuron (another logistic regression unit).  \n",
    "- This neuron takes the three values as input and outputs the **probability of being a top seller**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Neural Network Terminology\n",
    "- We group neurons into a **layer**:  \n",
    "  - A grouping of neurons that take as input the same/similar features and output numbers together.  \n",
    "- Layers can contain:  \n",
    "  - **Multiple neurons** (e.g., the first layer with affordability, awareness, perceived quality)  \n",
    "  - **Single neuron** (e.g., the final output neuron)  \n",
    "\n",
    "#### Key Layers\n",
    "- **Input layer (Layer 0)** ($\\bar{x}$) â†’ original features (e.g., price, shipping, marketing, material)  \n",
    "- **Hidden layers** ($\\bar{a}$) â†’ intermediate activations (not directly observed in training data)  \n",
    "- **Output layer** â†’ final probability of being a top seller  \n",
    "\n",
    "By convention, the input layer is layer 0 and when we say there are x layers, the input layer is not included in x.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Hidden Layers are \"Hidden\"\n",
    "- Training data includes:  \n",
    "  - Input values ($\\bar{x}$)  \n",
    "  - Output values ($\\bar{y}$)  \n",
    "- But it does **not** include the intermediate outputs of hidden layers ($\\bar{a}$).\n",
    "- Thatâ€™s why these layers are called **hidden**.\n",
    "- The output of hidden layers are the **activations**, which are defined as 'higher level features'.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Access in Practice\n",
    "- In practice, deciding which inputs belong to which neuron would be time-consuming.  \n",
    "- Instead:  \n",
    "  - Each neuron has access to **all features from the previous layer**.  \n",
    "  - Example: the affordability neuron can see all features but learns (through weights) to ignore marketing and material if theyâ€™re irrelevant.  \n",
    "\n",
    "---\n",
    "\n",
    "### Neural Networks as Feature Learning\n",
    "- Another way to view this neural network:  \n",
    "  - A **logistic regression model** with inputs = improved features (affordability, awareness, perceived quality).  \n",
    "  - These improved features are **learned** from the original inputs (price, shipping, marketing, material).  \n",
    "- This is similar to **feature engineering**:  \n",
    "  - Traditionally: we manually create features (e.g., $x_{1} \\cdot x_{2}$).  \n",
    "  - With neural networks: the model automatically **learns new features**.  \n",
    "\n",
    "ðŸ‘‰ **Note**: You donâ€™t have to manually create hidden-layer features â€” the model computes them during training.  \n",
    "You can have multiple hidden layers with multiple neurons.  \n",
    "How many hidden layers and how many neurons per layer are answered when you set up the **Neural Network Architecture**\n",
    "\n",
    "### Real Example: Face Recognition\n",
    "![face](facial_recog.png)\n",
    "- Say you have a 1000*1000 pixel picture.\n",
    "- This can be translated into a 1000*1000 matrix in code, with each value from 0-214 of how bright the pixel is.\n",
    "- This can be flattened into a 1 million length array ($1000 \\cdot 1000$) and inputted into the first layer.\n",
    "- One Layer may test for different types of lines in each neuron\n",
    "- The next may check for specific facial features, like eyes, nose, ears etc\n",
    "- The final hidden layer may aggregate the facial picture to find some sort of shape structure.\n",
    "- The ouput layer produces a probability of being a specific person.\n",
    "- The neural network would work out each of these hidden layers all by itself.\n",
    "- As we are progressing forwards through the activation values, this process is an example of **Forward Propagation Algorithm**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
