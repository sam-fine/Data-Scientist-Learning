{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3059b8-54c4-46d2-b446-49b76459c071",
   "metadata": {},
   "source": [
    "# Amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24112a-9c5f-454b-9321-bc46bd7104ca",
   "metadata": {},
   "source": [
    "# Amplitude Infrastructure Overview\n",
    "\n",
    "Amplitude provides a **collaborative, integrated, and self-service product suite** designed to bring data science and product analytics capabilities to both technical and non-technical users.  \n",
    "\n",
    "Its **Digital Optimization System** combines real-time data management, behavioral insights, experimentation, and recommendations to help teams continuously improve product experiences.\n",
    "\n",
    "---\n",
    "\n",
    "## Digital Optimization System\n",
    "\n",
    "![opt_system](optimisation_system.png)\n",
    "\n",
    "### Real-Time Data Management\n",
    "- **Plan, integrate, and govern** all customer and product data.  \n",
    "- Create a **single, secure, high-quality view of each customer**.\n",
    "\n",
    "### Behavioral Graph\n",
    "- The **brain of the system**: a purpose-built database for complex, iterative behavioral queries.  \n",
    "- Supports analysis of **historic and real-time data**, while enabling **prediction of future outcomes**.  \n",
    "\n",
    "---\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### 1. Amplitude Analytics\n",
    "Enables all teams to **explore and understand product usage**:  \n",
    "- Funnel analysis (Pathfinder)  \n",
    "- Retention analysis  \n",
    "- Feature engagement → discover what drives **Lifetime Value (LTV)**  \n",
    "- Cohort definitions for segmentation  \n",
    "- Dashboards and notebooks for visibility across teams  \n",
    "\n",
    "### 2. Amplitude Recommend\n",
    "Transforms insights into **personalized actions**:  \n",
    "- Identify target audiences  \n",
    "- Deliver tailored recommendations  \n",
    "\n",
    "### 3. Amplitude Experiment\n",
    "Empowers teams to **test and validate product decisions**:  \n",
    "- Hypotheses and A/B testing frameworks  \n",
    "- Configure and run user experience experiments  \n",
    "- Automate testing and iteration  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e0be1-d4a8-430b-975c-08ad33639bff",
   "metadata": {},
   "source": [
    "# Setting Up an Amplitude Experiment (A/B Test)\n",
    "\n",
    "Amplitude Experiments allow teams to **test product changes on real users** and measure their impact.  \n",
    "This ensures **data-driven decisions** while minimizing risk to the overall user experience.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Separate User Traffic\n",
    "- Each experiment must have its **own isolated user traffic**, so results are not contaminated by overlapping experiments.\n",
    "\n",
    "### 2. Feature Flags\n",
    "- Dynamically controllable **switches** that enable or disable features for specific user segments.  \n",
    "- Useful for rolling out new features gradually or targeting subsets of users.\n",
    "\n",
    "---\n",
    "\n",
    "## Creating an Experiment\n",
    "\n",
    "1. **Click `Create Experiment` → Feature**\n",
    "2. **Set up details:**  \n",
    "   - Name, description, relevant tags\n",
    "   - Optionally, specify a related product or feature\n",
    "\n",
    "3. **Design Experiment**\n",
    "   - **Define Goal (Hypothesis):**  \n",
    "     - Choose a primary metric to measure success (e.g., conversion rate, retention, engagement).  \n",
    "     - Specify **success criteria**: does the metric increase or decrease?  \n",
    "     - Set **Minimum Detectable Effect (MDE)** to ensure statistical power.\n",
    "   - **Additional Metrics:**  \n",
    "     - Track secondary metrics to monitor unintended consequences.  \n",
    "     - Only **one primary goal metric** is allowed.\n",
    "\n",
    "4. **Exposure Event**  \n",
    "   - Usually defaults to the key user action that triggers inclusion in the experiment.\n",
    "\n",
    "5. **Variants**\n",
    "   - **Control:** Baseline experience, often the current product behavior.  \n",
    "   - **Treatment:** New experience or change being tested.  \n",
    "   - Optional: multiple treatment groups for multivariate experiments.\n",
    "\n",
    "6. **Targeting**\n",
    "   - **Audience:**  \n",
    "     - All users, or a **specific cohort** defined by user properties or pre-existing segments.  \n",
    "   - **Distribution:**  \n",
    "     - Percent of audience in each variant. Typically 50/50 for simple A/B tests.  \n",
    "   - **Rollout:**  \n",
    "     - Percent of the total audience to be included in the experiment.  \n",
    "     - Useful for staged rollouts to minimize risk.\n",
    "\n",
    "---\n",
    "\n",
    "## Tips & Best Practices\n",
    "\n",
    "- Always **randomize assignment** to variants to avoid bias.  \n",
    "- Consider **sample size** and experiment duration before starting.  \n",
    "- Track both **primary and secondary metrics** to understand broader impact.  \n",
    "- Monitor for **overlapping experiments** that may affect results.  \n",
    "- Document the **hypothesis, target audience, and expected outcomes** before launching.  \n",
    "\n",
    "---\n",
    "\n",
    "## Reference\n",
    "- Amplitude Experiments Tutorial: [YouTube Video](https://www.youtube.com/watch?v=uUlJn8s5jd8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555b2e0-e98c-4aff-822c-3726e241c3f3",
   "metadata": {},
   "source": [
    "# Amplitude Experiment & A/B Test – Technical Notes\n",
    "\n",
    "Amplitude Experiments allow teams to **run controlled experiments on product changes** and measure impact using statistical analysis. Below are technical definitions, implementation notes, and statistical concepts used in experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Event Tracking (Implementation)\n",
    "\n",
    "Amplitude experiments rely on **instrumented events** to measure user behavior.  \n",
    "\n",
    "**Example:** Tracking a completed purchase in a web or backend application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c625f7-f894-4b07-8aca-adb7c83ced77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Python backend implementation of an Amplitude event\n",
    "from amplitude import Amplitude\n",
    "\n",
    "amplitude_client = Amplitude(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "# Track a purchase event\n",
    "amplitude_client.track(\n",
    "    user_id=str(12345),\n",
    "    event_type=\"Complete Purchase\",\n",
    "    event_properties={\n",
    "        \"total_price\": 59.99,\n",
    "        \"items_in_cart\": 3,\n",
    "        \"payment_method\": \"credit_card\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa0f2d-e9a8-4e7f-b255-4cbef5fdcacc",
   "metadata": {},
   "source": [
    "## 1. Event Implementation\n",
    "\n",
    "Amplitude events can be tracked in **frontend** or **backend** environments:\n",
    "\n",
    "**Frontend:** JavaScript, React, or mobile SDKs (iOS/Android)  \n",
    "**Backend:** Python, Node.js, or other server-side SDKs  \n",
    "\n",
    "**Purpose:**  \n",
    "- Events become **exposure or outcome events** for experiments.  \n",
    "- Allow Amplitude to **measure metrics** such as conversion, revenue, retention, or engagement.  \n",
    "\n",
    "**Notes:**  \n",
    "- `event_type` is the metric being measured.  \n",
    "- `event_properties` allow segmentation and cohort analysis.\n",
    "\n",
    "## 2. Hypothesis Testing Terminology\n",
    "\n",
    "Amplitude calculates **statistical results automatically** to evaluate experiment outcomes.\n",
    "\n",
    "| Term | Definition | Amplitude Use |\n",
    "|------|------------|---------------|\n",
    "| Hypothesis | Proposed change to test (e.g., \"Adding a new checkout button increases purchase rate\") | Defined during experiment setup; forms the basis for success metric |\n",
    "| Control Group | Users who see the current product experience | Baseline for comparison |\n",
    "| Treatment Group | Users exposed to the new feature | Compared against control to test hypothesis |\n",
    "| Metric | Measurable outcome (conversion, clicks, revenue) | Primary goal or additional metrics |\n",
    "| p-value | Probability of observing results as extreme as current under null hypothesis | Determines statistical significance |\n",
    "| t-score / z-score | Standardized difference between groups | Used internally to compute p-values |\n",
    "| Significance Level (α) | Threshold to reject null hypothesis (e.g., 0.05) | Defined in experiment settings |\n",
    "| Statistical Power (1-β) | Probability of detecting an effect if it exists | Used for sample size recommendations |\n",
    "| Minimum Detectable Effect (MDE) | Smallest effect size considered practically relevant | Set when defining experiment goals; affects sample size |\n",
    "\n",
    "## 3. Metrics & Significance in Amplitude\n",
    "\n",
    "**Statistical Significance:**  \n",
    "- Shows whether differences between control and treatment are **unlikely due to chance**.  \n",
    "- Shown in Amplitude as **confidence intervals, p-values, probability to beat control**.  \n",
    "\n",
    "**Practical Significance:**  \n",
    "- Indicates whether the effect size is **large enough to matter** in real-world product decisions.  \n",
    "- Often compared against **Minimum Detectable Effect (MDE)**.  \n",
    "\n",
    "**Where to see it:**  \n",
    "- **Experiment Dashboard:** control vs. treatment metrics, p-values, confidence intervals, effect size.  \n",
    "- **Secondary Metrics Table:** monitors additional metrics for unintended effects.\n",
    "\n",
    "# Confidence Intervals in A/B Testing\n",
    "\n",
    "A **confidence interval (CI)** is a range around your observed metric (e.g., conversion rate, click-through rate, revenue) that likely contains the **true metric value for the population**.\n",
    "\n",
    "- Typically expressed at a **95% confidence level**.  \n",
    "- This means: *if we ran the same experiment 100 times, 95 of those times the true effect would fall within this interval*.\n",
    "\n",
    "In **Amplitude Experiments**:  \n",
    "- Amplitude computes confidence intervals for **control and treatment groups**.  \n",
    "- They show **uncertainty around observed metrics**, helping you decide whether the treatment really has an effect.\n",
    "\n",
    "| Variant   | Conversion Rate | 95% CI        |\n",
    "| --------- | --------------- | ------------- |\n",
    "| Control   | 10.0%           | 8.0% – 12.0%  |\n",
    "| Treatment | 13.0%           | 11.0% – 15.0% |\n",
    "\n",
    "**Interpretation:**  \n",
    "- The **observed difference** between control (10%) and treatment (13%) is 3%.  \n",
    "- The **true difference** could vary between roughly 1% and 7%.  \n",
    "- If the **confidence intervals do not overlap**, it often suggests **statistical significance**.\n",
    "\n",
    "## 4. Where Statistics Are Applied\n",
    "\n",
    "**During Experiment Analysis:**  \n",
    "- Amplitude computes:  \n",
    "  - Differences between control and treatment metrics  \n",
    "  - p-values, confidence intervals, t/z scores  \n",
    "  - Probability of beating control  \n",
    "\n",
    "**Before Experiment Launch (Optional):**  \n",
    "- Power Analysis to determine **required sample size** for detecting MDE  \n",
    "- Avoids underpowered experiments that fail to detect real effects\n",
    "\n",
    "## 5. Implementation Notes & Best Practices\n",
    "\n",
    "**Events:**  \n",
    "- Instrument all relevant user actions using `amplitude_client.track`.\n",
    "\n",
    "**Experiment Setup:**  \n",
    "- Define goal, variants, audience, rollout, and exposure events.\n",
    "\n",
    "**Analysis:**  \n",
    "- Check dashboard for statistical significance  \n",
    "- Compare effect size to MDE for practical significance  \n",
    "- Monitor secondary metrics for unintended effects\n",
    "\n",
    "**Best Practices:**  \n",
    "- Avoid overlapping experiments on the same cohort  \n",
    "- Use random assignment for control/treatment  \n",
    "- Monitor experiment duration to avoid temporal biases\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
